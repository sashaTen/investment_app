
https://raw.githubusercontent.com/sardarosama/Stock-Market-Trend-Prediction-Using-Sentiment-Analysis/refs/heads/main/stock_data.csv
*essential core APIs of MLflow Tracking:
logging, registering, and loading of a model for inference.

*you can    log   hyperparams  and   loss  metrics .

* also   you can  save the  vectoriaer  for  text  preproccessing  

* you  need  to  look  up  what  is   important  to log 
in order  to recreate  the   experements 

* you  can   just   download the  model  and    artifacts and then 
just  use   paths   to  your  local  machine   for  reference   

* Tuple[pd.Series, pd.Series, pd.Series, pd.Series]   is   
X_train, X_test, y_train, y_test

*  what   you  return  is  the   output
in the  zen ml  server 

*then   you   train  the   pipeline  and 
you can  use  the models    by loading  in  similar    
way  you do  with   mlflow : 
 vectorizer_artifact = Client().get_artifact_version('ae428915-e9f8-46ad-80a3-f223ebb4e6ce')
    vectorizer = vectorizer_artifact.load()

    model_artifact = Client().get_artifact_version('6a4a2d3b-8ba9-4248-9c4e-752080717532')
    model = model_artifact.load()
    sentiment = request.POST['sentiment']
    sentiment =  [sentiment]
    sentiment_vectorized = vectorizer.transform(sentiment)
    prediction = model.predict(sentiment_vectorized)


*    how to    make    autoretrain  model   ? 
def load_current_vectorizer_and_model():
    my_runs_on_current_stack = client.list_pipeline_runs(
        stack_id=client.active_stack_model.id,
        user_id=client.active_user.id,
        sort_by="desc:start_time",
        size=10,
    )

load the   pipelines 
the by  id   find  the  latest.
then load the  step   correctly.
then   look  at    server  and load the  output.


  '''
    result = subprocess.run(
        ['python', r'C:\Users\HP\Desktop\stock_app\invest\invest_app\util_functions.py'],  # Use raw string or fix path
        capture_output=True,
        text=True
    )
    print(result)
    '''
--------

    def    testing(request):
    url = 'https://raw.githubusercontent.com/surge-ai/stock-sentiment/main/sentiment.csv'
    df =  load_data(url)
    #auto_retrain_on_new_data(df)
    for index, row in df.iterrows():
      TweetSentiment.objects.create(
        tweet_text=row['Tweet Text'],
        sentiment=row['Sentiment']
    )

    return   HttpResponse(df.head(0))
    --------------

    -------------grow   numbrt  of  sample   count 
    .
     sample_count =  Count_samples_for_retrain.objects.first()
    sample_count.samples_number= sample_count.samples_number+1
    sample_count.save()
    ----------

    nest  step   maybe the  cheking the new  model from  experementations 



    ***********
    log_return = np.log(1 + data.pct_change())

# Generate Random Weights.
random_weights = np.array(np.random.random(len(tickers)))

# Generate the Rebalance Weights, these should equal 1.
rebalance_weights = random_weights / np.sum(random_weights)

# Calculate the Expected Returns, annualize it by multiplying it by `252`.
exp_ret = np.sum((log_return.mean() * rebalance_weights) * 252)

# Calculate the Expected Volatility, annualize it by multiplying it by `252`.
exp_vol = np.sqrt(
np.dot(
    rebalance_weights.T,
    np.dot(
        log_return.cov() * 252,
        rebalance_weights
    )
)
)

# Calculate the Sharpe Ratio.
sharpe_ratio = exp_ret / exp_vol
print(sharpe_ratio)



weights_df = pd.DataFrame(data={
'random_weights': random_weights,
'rebalance_weights': rebalance_weights
})


# Do the same with the other metrics.
metrics_df = pd.DataFrame(data={
    'Expected Portfolio Returns': exp_ret,
    'Expected Portfolio Volatility': exp_vol,
    'Portfolio Sharpe Ratio': sharpe_ratio
}, index=[0])


num_of_portfolios = 50

# Prep an array to store the weights as they are generated, 5000 iterations for each of our 4 symbols.
all_weights = np.zeros((num_of_portfolios, len(tickers)))

# Prep an array to store the returns as they are generated, 5000 possible return values.
ret_arr = np.zeros(num_of_portfolios)

# Prep an array to store the volatilities as they are generated, 5000 possible volatility values.
vol_arr = np.zeros(num_of_portfolios)

# Prep an array to store the sharpe ratios as they are generated, 5000 possible Sharpe Ratios.
sharpe_arr = np.zeros(num_of_portfolios)

# Start the simulations.
for ind in range(num_of_portfolios):

    # First, calculate the weights.
    weights = np.array(np.random.random( len(tickers)))
    weights = weights / np.sum(weights)

    # Add the weights, to the `weights_arrays`.
    all_weights[ind, :] = weights

    # Calculate the expected log returns, and add them to the `returns_array`.
    ret_arr[ind] = np.sum((log_return.mean() * weights) * 252)

    # Calculate the volatility, and add them to the `volatility_array`.
    vol_arr[ind] = np.sqrt(
        np.dot(weights.T, np.dot(log_return.cov() * 252, weights))
    )

    # Calculate the Sharpe Ratio and Add it to the `sharpe_ratio_array`.
    sharpe_arr[ind] = ret_arr[ind]/vol_arr[ind]

# Let's create our "Master Data Frame", with the weights, the returns, the volatility, and the Sharpe Ratio
simulations_data = [ret_arr, vol_arr, sharpe_arr, all_weights]

# Create a DataFrame from it, then Transpose it so it looks like our original one.
simulations_df = pd.DataFrame(data=simulations_data).T

# Give the columns the Proper Names.
simulations_df.columns = [
    'Returns',
    'Volatility',
    'Sharpe Ratio',
    'Portfolio Weights'
]

# Make sure the data types are correct, we don't want our floats to be strings.
simulations_df = simulations_df.infer_objects()

# Print out the results.
print('')
print('='*80)
print('SIMULATIONS RESULT:')
print('-'*80)
print(simulations_df.head())
print('-'*80)