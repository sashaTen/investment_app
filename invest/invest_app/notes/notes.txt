
https://raw.githubusercontent.com/sardarosama/Stock-Market-Trend-Prediction-Using-Sentiment-Analysis/refs/heads/main/stock_data.csv
*essential core APIs of MLflow Tracking:
logging, registering, and loading of a model for inference.

*you can    log   hyperparams  and   loss  metrics .

* also   you can  save the  vectoriaer  for  text  preproccessing  

* you  need  to  look  up  what  is   important  to log 
in order  to recreate  the   experements 

* you  can   just   download the  model  and    artifacts and then 
just  use   paths   to  your  local  machine   for  reference   

* Tuple[pd.Series, pd.Series, pd.Series, pd.Series]   is   
X_train, X_test, y_train, y_test

*  what   you  return  is  the   output
in the  zen ml  server 

*then   you   train  the   pipeline  and 
you can  use  the models    by loading  in  similar    
way  you do  with   mlflow : 
 vectorizer_artifact = Client().get_artifact_version('ae428915-e9f8-46ad-80a3-f223ebb4e6ce')
    vectorizer = vectorizer_artifact.load()

    model_artifact = Client().get_artifact_version('6a4a2d3b-8ba9-4248-9c4e-752080717532')
    model = model_artifact.load()
    sentiment = request.POST['sentiment']
    sentiment =  [sentiment]
    sentiment_vectorized = vectorizer.transform(sentiment)
    prediction = model.predict(sentiment_vectorized)


*    how to    make    autoretrain  model   ? 
def load_current_vectorizer_and_model():
    my_runs_on_current_stack = client.list_pipeline_runs(
        stack_id=client.active_stack_model.id,
        user_id=client.active_user.id,
        sort_by="desc:start_time",
        size=10,
    )

load the   pipelines 
the by  id   find  the  latest.
then load the  step   correctly.
then   look  at    server  and load the  output.


  '''
    result = subprocess.run(
        ['python', r'C:\Users\HP\Desktop\stock_app\invest\invest_app\util_functions.py'],  # Use raw string or fix path
        capture_output=True,
        text=True
    )
    print(result)
    '''
--------

    def    testing(request):
    url = 'https://raw.githubusercontent.com/surge-ai/stock-sentiment/main/sentiment.csv'
    df =  load_data(url)
    #auto_retrain_on_new_data(df)
    for index, row in df.iterrows():
      TweetSentiment.objects.create(
        tweet_text=row['Tweet Text'],
        sentiment=row['Sentiment']
    )

    return   HttpResponse(df.head(0))
    --------------

    -------------grow   numbrt  of  sample   count 
    .
     sample_count =  Count_samples_for_retrain.objects.first()
    sample_count.samples_number= sample_count.samples_number+1
    sample_count.save()
    ----------